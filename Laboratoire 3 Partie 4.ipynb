{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rappel Google Colab\n",
    "\n",
    "Tout d'abord, sélectionnez l'option GPU de Colab avec *Edit > Notebook settings* et sélectionner GPU comme Hardware accelerator. \n",
    "Installer ensuite deeplib avec la commande suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ulaval-damas/glo4030-labs.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratoire 3: Optimisation\n",
    "\n",
    "## Partie 4: Initialisation des poids\n",
    "\n",
    "Dans cette section, vous testerez différentes techniques d'initialisations et observerez leurs effets sur le gradient et l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from deeplib.datasets import train_valid_loaders\n",
    "from deeplib.training import train, test\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "from deeplib.datasets import load_mnist, load_cifar10\n",
    "cifar_train, cifar_test = load_cifar10()\n",
    "cifar_train.transform = ToTensor()\n",
    "cifar_test.transform = ToTensor()\n",
    "\n",
    "train_loader, valid_loader = train_valid_loaders(cifar_train, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée ici un réseau de neurones assez simple composé de 5 couches cachées (6 couches au total) et avec un choix pour la fonction d'activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = dict(\n",
    "    tanh=nn.Tanh,\n",
    "    relu=nn.ReLU\n",
    ")\n",
    "\n",
    "def create_fully_connected_network(activation):\n",
    "    assert activation in activations\n",
    "    activation = activations[activation]\n",
    "    num_neurons = 1000\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32*32*3, num_neurons),\n",
    "        activation(),\n",
    "        nn.Linear(num_neurons, num_neurons),\n",
    "        activation(),\n",
    "        nn.Linear(num_neurons, num_neurons),\n",
    "        activation(),\n",
    "        nn.Linear(num_neurons, num_neurons),\n",
    "        activation(),\n",
    "        nn.Linear(num_neurons, num_neurons),\n",
    "        activation(),\n",
    "        nn.Linear(num_neurons, 10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va jouer avec différentes fonctions d'initialisation. Créons donc une fonction nous permettant d'initialiser tous les poids de notre réseau de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_network(network, initialization_function):\n",
    "    for module in network.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            initialization_function(module.weight)\n",
    "            init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse aux gradients qui circulent dans le réseau de neurones lors de la rétropropagation. Ceci est à distinguer du gradient calculé pour chacun des poids individuels du réseau de neurones. Le gradient circulant pendant la rétropropagation nous donne une idée de la possibilité de changements des poids de la couche en question. De manière équivalente, le gradient qui circule dans le réseau est le même que celui des biais des couches linéaires. Les fonctions suivantes procèdent donc de la façon suivante:\n",
    "- On parcourt le jeu de données d'entraînement en batch;\n",
    "- Pour chacune des batchs, on garde pour chacune des couches le gradient des biais de la couche;\n",
    "- Une fois que toutes les batchs ont été traitées, on calcule un histogramme des gradients pour chaque couche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def save_gradient(sequential_network, output_dictionary):\n",
    "    layer_number = 1\n",
    "    for layer in sequential_network:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            # On ignore la dernière couche qui est la couche de \n",
    "            # classification.\n",
    "            if layer_number == 6:\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                grad = layer.bias.grad.flatten().cpu().numpy()\n",
    "            grad = grad[grad != 0]\n",
    "            output_dictionary[layer_number].append(grad)\n",
    "            layer_number += 1\n",
    "\n",
    "def plot_gradients_per_layer(gradients_per_layer):\n",
    "    for layer_number, grads in gradients_per_layer.items():\n",
    "        grad = np.concatenate(grads)\n",
    "        hist, bin_edges = np.histogram(grad, bins=100)\n",
    "        hist = hist / hist.sum() * 100\n",
    "\n",
    "        plt.plot(bin_edges[:-1], hist, label=f'Layer {layer_number}')\n",
    "\n",
    "def plot_gradient(network):\n",
    "    gradients_per_layer = defaultdict(list)\n",
    "    network.cuda()\n",
    "    for x, y in train_loader:\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        output = network(x)\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        save_gradient(network, gradients_per_layer)\n",
    "\n",
    "        network.zero_grad(True)\n",
    "\n",
    "    plot_gradients_per_layer(gradients_per_layer)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ci-dessous est la fonction qui est utilisée comme référence dans [l'article introduisant l'initialisation Glorot/Xavier](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) et est d'ailleurs celle utilisé par PyTorch par défaut dans les couches linéaires. Nous allons l'utiliser pour la comparer avec l'initialisation de Glorot/Xavier et celle de Kaiming He."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_uniform(weight):\n",
    "    bound = 1. / np.sqrt(weight.shape[1])\n",
    "    init.uniform_(weight, -bound, bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Réseau avec activation tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction tanh était la fonction d'activation la plus utilisée avant l'arrivée de la fonction ReLU. Plusieurs fonctions d'initialisation ont donc été conçues avec cette fonction d'activation en tête. Investiguons donc l'effet des différentes fonctions d'initialisation sur un réseau avec des activations tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_network = create_fully_connected_network('tanh')\n",
    "tanh_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons l'histogramme des gradients lorsqu'on utilise l'initialisation standard (de référence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(tanh_network, standard_uniform)\n",
    "\n",
    "plot_gradient(tanh_network)\n",
    "plt.title('Standard uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons l'histogramme des gradients lorsqu'on utilise l'initialisation de Glorot/Xavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(tanh_network, init.xavier_uniform_)\n",
    "\n",
    "plot_gradient(tanh_network)\n",
    "plt.title('Xavier uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons l'histogramme des gradients lorsqu'on utilise l'initialisation de Kaiming He."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(tanh_network, init.kaiming_uniform_)\n",
    "\n",
    "plot_gradient(tanh_network)\n",
    "plt.title('Kaiming uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- À partir des graphiques pour chacune des fonctions d'initialisation, que peut-on dire sur la différence d'initialisation entre les différents types d'initialisation?\n",
    "- Intuitivement, pourquoi serait-il préférable d'avoir une variance similaire pour le gradient circulant dans chacune des couches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a observé l'effet de l'initialisation sur le gradient circulant dans le réseau, regardons si l'effet est répercuté sur l'apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînons le réseau avec l'initialisation standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(tanh_network, standard_uniform)\n",
    "\n",
    "history = train(tanh_network, 'sgd', cifar_train, epochs, batch_size)\n",
    "history.display()\n",
    "print('Exactitude en test: {:.2f}'.format(test(tanh_network, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant l'histogramme des gradients du réseau entraîné avec l'initialisation standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient(tanh_network)\n",
    "plt.title('Trained tanh network with standard uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Quelle différence remarquez-vous par rapport à l'histogramme du gradient circulant de l'initialisation standard avant l'entraînement (Voir le graphique \"Standard uniform\" plus haut) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînons le réseau avec l'initialisation Xavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(tanh_network, init.xavier_uniform_)\n",
    "\n",
    "history = train(tanh_network, 'sgd', cifar_train, epochs, batch_size)\n",
    "history.display()\n",
    "print('Exactitude en test: {:.2f}'.format(test(tanh_network, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant l'histogramme des gradients du réseau entraîné avec l'initialisation Xavier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient(tanh_network)\n",
    "plt.title('Trained tanh network with Xavier uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînons le réseau avec l'initialisation Kaiming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initialize_network(tanh_network, init.kaiming_uniform_)\n",
    "\n",
    "history = train(tanh_network, 'sgd', cifar_train, epochs, batch_size)\n",
    "history.display()\n",
    "print('Exactitude en test: {:.2f}'.format(test(tanh_network, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant l'histogramme des gradients du réseau entraîné avec l'initialisation Kaiming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient(tanh_network)\n",
    "plt.title('Trained tanh network with Kaiming uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Que notez-vous en termes de performances des différentes techniques d'initialisation ?\n",
    "- Comparez les graphiques pour les initialisations Xavier et Kaiming avant et après entraînement. Que remarquez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Réseau avec activation ReLU\n",
    "\n",
    "Effectuons donc le même processus mais c'est fois-ci avec la fonction ReLU. \n",
    "\n",
    ">Notons que la fonction calculant les histogrammes enlève tous les gradients qui sont exactement à zéro. Autrement, chaque histogramme aurait un grand pic à zéro nous empêchant de voir la distribution du reste des gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu_network = create_fully_connected_network('relu')\n",
    "relu_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(relu_network, standard_uniform)\n",
    "\n",
    "plot_gradient(relu_network)\n",
    "plt.title('Standard uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(relu_network, init.xavier_uniform_)\n",
    "\n",
    "plot_gradient(relu_network)\n",
    "plt.title('Xavier uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(relu_network, init.kaiming_uniform_)\n",
    "\n",
    "plot_gradient(relu_network)\n",
    "plt.title('Kaiming uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectuons les entraînements avec les différentes fonctions d'initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initialize_network(relu_network, standard_uniform)\n",
    "\n",
    "history = train(relu_network, 'sgd', cifar_train, epochs, batch_size)\n",
    "history.display()\n",
    "print('Exactitude en test: {:.2f}'.format(test(relu_network, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient(relu_network)\n",
    "plt.title('Trained ReLU network with standard uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(relu_network, init.xavier_uniform_)\n",
    "\n",
    "history = train(relu_network, 'sgd', cifar_train, epochs, batch_size)\n",
    "history.display()\n",
    "print('Exactitude en test: {:.2f}'.format(test(relu_network, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient(relu_network)\n",
    "plt.title('Trained ReLU network with Xavier uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_network(relu_network, init.kaiming_uniform_)\n",
    "\n",
    "history = train(relu_network, 'sgd', cifar_train, epochs, batch_size)\n",
    "history.display()\n",
    "print('Exactitude en test: {:.2f}'.format(test(relu_network, cifar_test, batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient(relu_network)\n",
    "plt.title('Trained ReLU network with Kaiming uniform')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "- Quelles similarités remarquez-vous en termes de performance et de gradient entre le réseau avec activation tanh et le réseau avec activation ReLU ?\n",
    "- Quelles différences remarquez-vous en termes de performance et de gradient entre le réseau avec activation tanh et le réseau avec activation ReLU ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
