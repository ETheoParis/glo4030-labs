{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a98f0bb",
   "metadata": {},
   "source": [
    "### Utilisation de Google Colab\n",
    "\n",
    "Si vous utilisez Google Colab, suivez les instructions ci-dessous.\n",
    "\n",
    "Tout d'abord, sélectionnez l'option GPU de Colab avec *Edit > Notebook settings* et sélectionner GPU comme Hardware accelerator. Installer ensuite deeplib avec la commande suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f620828",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ulaval-damas/glo4030-labs.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78915c",
   "metadata": {},
   "source": [
    "# Laboratoire 8: Transformers\n",
    "\n",
    "Introduction aux transformers:\n",
    "- Révolution dans le monde du NLP (Vaswani, 2017), maintenant les majeurs architectures de NLP sont toutes basées sur les transformers (BERT, GPT). Ex. Generative Pre-trained Transformer (GPT) et Bidirectional Encoder Representations from Transformers (BERT). ChatGPT basé sur GPT.\n",
    "- Maintenant, les transformers sont aussi utilisés dans d'autres domaines du deep learning grâce à leur grande flexibilité et polyvalence. Vision par ordinateur: ViT, Swin, DETR, etc., Point Clouds: ..., Audio: ...\n",
    "- Intéressant que le transformer de 2017 est encore utilisé! Seulement quelques petites modifications\n",
    "- Une sorte d'architecture universelle qui peut traiter tout type de données\n",
    "- Présentent une affinité pour le prétraining en raison de leur meilleur scaling avec la quantité de données et la taille du modèle (scaling vision transformer et Scaling laws for neural language models)\n",
    "\n",
    "Transformers:\n",
    "- Les transformers sont basés sur le principle d'attention\n",
    "- Modèle ensemble -> ensemble de vecteurs (ensemble dans le sens mathématique)\n",
    "- Réutilise des concepts intéressants: Connexion résiduelle (flot du gradient), LayerNorm, MLP\n",
    "- Liens avec le message passing / algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae00f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14cd820",
   "metadata": {},
   "source": [
    "## Partie 1: Scaled Dot-Product Attention\n",
    "- Au coeur des transformers se trouve le principle d'attention\n",
    "- Focus sur certaines parties de l'information, et moins sur d'autres\n",
    "- Dans le cas des transformers, on utilise le \"Scaled Dot-Product Attention\" de Attention Is All You Need\" (Vaswani, 2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4bc421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff13df80",
   "metadata": {},
   "source": [
    "### Champ réceptif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4340ab",
   "metadata": {},
   "source": [
    "### Attention comme du passage de messages\n",
    "- Graphe\n",
    "- Q, K, aggregate V\n",
    "- Graphe ont pas de positions -> fonction set de vecteurs vers set de vecteurs , doit encoder la position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29997f",
   "metadata": {},
   "source": [
    "### Self-attention vs cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71e459",
   "metadata": {},
   "source": [
    "### Le \"Scaled\" dans Scaled Dot-Product Attention\n",
    "- Si k et v ~ N(0, 1)\n",
    "- q @ k.T -> variance de d\n",
    "- scaled par 1 / sqrt(d)\n",
    "- empeche softmax de converger vers one-hot, aggrege information de seulement un token, plutot que plusieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5673a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9649292230606079\n",
      "0.9273311495780945\n",
      "63.43318557739258\n",
      "0.991143524646759\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "num_tokens = 16\n",
    "dimension = 64\n",
    "\n",
    "q = torch.randn(batch_size, num_tokens, dimension)\n",
    "k = torch.randn(batch_size, num_tokens, dimension)\n",
    "attention = q @ k.transpose(-2, -1)\n",
    "scaled_attention = attention * dimension ** -0.5\n",
    "\n",
    "print(f'{k.var()}')\n",
    "print(f'{v.var()}')\n",
    "print(f'{attention.var()}')\n",
    "print(f'{scaled_attention.var()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "410d95d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
      "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
     ]
    }
   ],
   "source": [
    "attention = torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n",
    "scaled_attention = torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * np.sqrt(dimension), dim=-1)\n",
    "\n",
    "print(attention)\n",
    "print(scaled_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc55a34",
   "metadata": {},
   "source": [
    "## Partie 2: Multi-Head Attention\n",
    "- Plusieurs têtes d'attention\n",
    "- Plusieurs cannaux de communication séparés entre les noeuds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d992fcb",
   "metadata": {},
   "source": [
    "## Partie 3: TransformerEncoderLayer\n",
    "- Multi-head self-attention\n",
    "- Add and norm\n",
    "- FF par token (attention = communication, FF est le token qui \"pense\")\n",
    "- Connexion residuelle / skip (flot du gradient car l'addition distribue le grad egalement à chaque branche, aide à être deep, apprendre un delta)\n",
    "- Prenorm vs postnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d0a0a",
   "metadata": {},
   "source": [
    "### BatchNorm vs LayerNorm\n",
    "- LayerNorm ne demande pas de moyenne exponentielle, pas de difference entre train et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d0d9c412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm\n",
      "Par batch:\n",
      "\tmean: -1.4901161193847656e-08\n",
      "\tvar: 1.0\n",
      "Par layer:\n",
      "\tmean: 0.026868706569075584\n",
      "\tvar: 1.249971628189087\n",
      "LayerNorm\n",
      "Par batch:\n",
      "\tmean: 1.2997647523880005\n",
      "\tvar: 0.41209426522254944\n",
      "Par layer:\n",
      "\tmean: 2.38418573772492e-09\n",
      "\tvar: 1.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 32\n",
    "dimension = 100\n",
    "x = torch.randn(batch_size, dimension)\n",
    "net = nn.Sequential(nn.Linear(100, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, 100))\n",
    "x = net(x)\n",
    "\n",
    "# BatchNorm (normalement utiliserait une moyenne exponentielle)\n",
    "xmean_batch = x.mean(0, keepdim=True)\n",
    "xvar_batch = x.var(0, keepdim=True)\n",
    "x_batch = (x - xmean_batch) / torch.sqrt(xvar_batch + 1e-12)\n",
    "print('BatchNorm')\n",
    "print(f'Par batch:\\n\\tmean: {x_batch[:, 0].mean()}\\n\\tvar: {x_batch[:, 0].var()}')\n",
    "print(f'Par layer:\\n\\tmean: {x_batch[0, :].mean()}\\n\\tvar: {x_batch[0, :].var()}')\n",
    "\n",
    "# LayerNorm\n",
    "xmean_layer = x.mean(-1, keepdim=True)\n",
    "xvar_layer = x.var(-1, keepdim=True)\n",
    "x_layer = (x - xmean_layer) / torch.sqrt(xvar_layer + 1e-12)\n",
    "print('LayerNorm')\n",
    "print(f'Par batch:\\n\\tmean: {x_layer[:, 0].mean()}\\n\\tvar: {x_layer[:, 0].var()}')\n",
    "print(f'Par layer:\\n\\tmean: {x_layer[0, :].mean()}\\n\\tvar: {x_layer[0, :].var()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea36f88",
   "metadata": {},
   "source": [
    "## Partie 4: TransformerEncoder\n",
    "- Plusieurs couches de TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2074c3",
   "metadata": {},
   "source": [
    "## Partie 5: Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb791682",
   "metadata": {},
   "source": [
    "## Partie 6: Vision Transformer\n",
    "- Utilise PyTorch car plus efficace\n",
    "- https://towardsdatascience.com/a-demonstration-of-using-vision-transformers-in-pytorch-mnist-handwritten-digit-recognition-407eafbc15b0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
